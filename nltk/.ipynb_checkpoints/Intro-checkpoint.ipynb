{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4790cf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kushidhar/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('brown')\n",
    "# nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2953e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c74b6c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5da6f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = brown.sents(categories='adventure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e825c08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan Morgan told himself he would forget Ann Turner .\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c9cb0f",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "- Get the Data/Corpus\n",
    "- Tokenisation,Stopword Removal\n",
    "- Stemming\n",
    "- Building a vocab\n",
    "- Vectorization\n",
    "- Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98186791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "969be3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"\n",
    "It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout.\n",
    "The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters,\n",
    "as opposed to using 'Content here, content here', making it look like readable English. Many desktop publishing packages and\n",
    "web page editors now use Lorem Ipsum as their default model text, and a search for 'lorem ipsum' will uncover many web sites still in their infancy.\n",
    "Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce5cdf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Lorem Ipsum has been the industry's standard dummy text ever since 1,2,3 the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd8f7645",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01c3115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20008e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b738ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word\n",
    "# sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a8899",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6cad97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "37b53cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "len(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "292a2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    sws = set(stopwords.words('english'))\n",
    "    negative_stopwords = [\"against\",\"aren\", \"aren't\",\"couldn't\",\"didn't\",\"doesn\",\n",
    "        \"doesn't\",\"don\",\"don't\",\"hadn\",\"hadn't\",\"haven\",\"haven't\",\n",
    "        \"isn\",\"isn't\", \"mightn\",\"mightn't\",\"mustn\",\"mustn't\",\"needn\",\n",
    "        \"needn't\",\"no\",\"nor\",\"not\", \"shan\",\"shan't\", \"shouldn\",\"shouldn't\",\n",
    "        \"wasn\",\"wasn't\",\"weren\",\"weren't\",\"wouldn\",\"wouldn't\"]\n",
    "    for word in negative_stopwords:\n",
    "        sws.remove(word)\n",
    "    text = word_tokenize(text)\n",
    "    useful_words = [w for w in text if w not in sws]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "03e884d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i am not 12,3 cool and was't cool #not @guys\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5f60949f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not', '12,3', 'cool', \"was't\", 'cool', 'not', 'guys']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_tokenize(text)\n",
    "remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7a8208f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d91fca",
   "metadata": {},
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8650e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db01885f",
   "metadata": {},
   "source": [
    "<img src = \"regex1.png\"  width=\"400\" height=\"600\" align=\"left\"/>\n",
    "<img src = \"regex2.png\"  width=\"400\" height=\"800\" align=\"right\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d934b0b3",
   "metadata": {},
   "source": [
    "<img src = \"regex3.png\"  width=\"400\" height=\"600\" align=\"left\"/>\n",
    "<img src = \"regex4.png\"  width=\"400\" height=\"1000\" align=\"right\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7a5b1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[a-zA-Z@'.]+\")\n",
    "useful_text = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb3b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb18bb68",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "- process that transforms particular words(verbs,plurals) into their radical form\n",
    "- preserve the semantics of the scentence without increasing the number of unique tokens\n",
    "- ex : jumps,jumping,jumped,jump ==> jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "823176a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The quick brown fox jumps over the lazy dog\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cf6989bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer,PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "# SnowballStemmer,PorterStemmer,LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d4769a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'బౌలర్లపై ద్రావిడ్ ప్రత్యేక దృష్టి'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "# avaliable in multi languages\n",
    "ps.stem('బౌలర్లపై ద్రావిడ్ ప్రత్యేక దృష్టి')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e9dc196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only english ye mawa\n",
    "ss = SnowballStemmer('english')\n",
    "ss.stem('Cooling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08f3d7",
   "metadata": {},
   "source": [
    "## Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "776286c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'em chestundho em ardham kavadam ledhu ద్రావిడ్'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wn = WordNetLemmatizer()\n",
    "wn.lemmatize('em chestundho em ardham kavadam ledhu ద్రావిడ్')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "62303432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b1152",
   "metadata": {},
   "source": [
    "# Building a Vocab and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4ae31805",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"At vero eos et accusamus et iusto odio dignissimos sentium voluptatum deleniti atqueerunt mollitia animi, id est laborum et dolorum fuga\",\n",
    "    \" Et harum quidem rerum facilis est et expedita distinctio\",\n",
    "    \" Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus\",\n",
    "    \"Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae\",\n",
    "    \" Itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fd66f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f7a7df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0a92eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorzed_corpus = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6c2a1500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorzed_corpus.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8b58e8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        3, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        2, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorzed_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d77250e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'at': 4, 'vero': 70, 'eos': 20, 'et': 22, 'accusamus': 0, 'iusto': 33, 'odio': 46, 'dignissimos': 14, 'sentium': 63, 'voluptatum': 73, 'deleniti': 13, 'atqueerunt': 5, 'mollitia': 40, 'animi': 2, 'id': 30, 'est': 21, 'laborum': 34, 'dolorum': 17, 'fuga': 27, 'harum': 28, 'quidem': 53, 'rerum': 60, 'facilis': 26, 'expedita': 24, 'distinctio': 15, 'nam': 41, 'libero': 35, 'tempore': 66, 'cum': 9, 'soluta': 65, 'nobis': 44, 'eligendi': 19, 'optio': 48, 'cumque': 10, 'nihil': 43, 'impedit': 31, 'quo': 54, 'minus': 38, 'quod': 55, 'maxime': 37, 'placeat': 50, 'facere': 25, 'possimus': 51, 'temporibus': 67, 'autem': 7, 'quibusdam': 52, 'aut': 6, 'officiis': 47, 'debitis': 11, 'necessitatibus': 42, 'saepe': 61, 'eveniet': 23, 'ut': 69, 'voluptates': 71, 'repudiandae': 59, 'sint': 64, 'molestiae': 39, 'non': 45, 'recusandae': 56, 'itaque': 32, 'earum': 18, 'hic': 29, 'tenetur': 68, 'sapiente': 62, 'delectus': 12, 'reiciendis': 57, 'voluptatibus': 72, 'maiores': 36, 'alias': 1, 'consequatur': 8, 'perferendis': 49, 'doloribus': 16, 'asperiores': 3, 'repellat': 58}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2245c5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorzed_corpus.toarray()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7d5cb9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorze_corpus =  vectorzed_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d0a5e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = vectorze_corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ed2b1389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "098a5eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['cum', 'cumque', 'eligendi', 'est', 'facere', 'id', 'impedit',\n",
       "        'libero', 'maxime', 'minus', 'nam', 'nihil', 'nobis', 'optio',\n",
       "        'placeat', 'possimus', 'quo', 'quod', 'soluta', 'tempore'],\n",
       "       dtype='<U14')]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "65e43596",
   "metadata": {},
   "outputs": [],
   "source": [
    "randnums= np.random.randint(0,6,74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "aed23c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c84d420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = cv.inverse_transform(randnums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "02566e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['accusamus', 'alias', 'animi', 'asperiores', 'at', 'atqueerunt',\n",
       "        'aut', 'autem', 'cum', 'debitis', 'delectus', 'dignissimos',\n",
       "        'distinctio', 'doloribus', 'dolorum', 'earum', 'eligendi', 'eos',\n",
       "        'est', 'et', 'eveniet', 'expedita', 'facere', 'fuga', 'harum',\n",
       "        'hic', 'id', 'impedit', 'iusto', 'laborum', 'maiores', 'maxime',\n",
       "        'minus', 'molestiae', 'mollitia', 'nam', 'nihil', 'nobis', 'non',\n",
       "        'odio', 'officiis', 'optio', 'perferendis', 'placeat', 'possimus',\n",
       "        'quibusdam', 'quidem', 'quo', 'quod', 'recusandae', 'reiciendis',\n",
       "        'repellat', 'repudiandae', 'rerum', 'saepe', 'sapiente', 'sentium',\n",
       "        'sint', 'soluta', 'tempore', 'temporibus', 'tenetur', 'ut',\n",
       "        'voluptatibus', 'voluptatum'], dtype='<U14')]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b1233f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accusamus alias animi asperiores at atqueerunt aut autem cum debitis delectus dignissimos distinctio doloribus dolorum earum eligendi eos est et eveniet expedita facere fuga harum hic id impedit iusto laborum maiores maxime minus molestiae mollitia nam nihil nobis non odio officiis optio perferendis placeat possimus quibusdam quidem quo quod recusandae reiciendis repellat repudiandae rerum saepe sapiente sentium sint soluta tempore temporibus tenetur ut voluptatibus voluptatum'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b9773039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(document):\n",
    "    \n",
    "    words = tokenizer.tokenize(document.lower())\n",
    "    # remove stopwords\n",
    "    words = ' '.join(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3c966d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"jaii balayya jai jai balayya \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ceb0fa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jaii', 'balayya', 'jai', 'jai', 'balayya']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTokenizer(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b472b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer = myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c6712798",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e740f423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "0f9527e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [\"Holaa animi lorem golla\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "eef7bb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(test_corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58508963",
   "metadata": {},
   "source": [
    "## More ways to create features\n",
    "- Unigram -every word as a feature\n",
    "- Bigrams\n",
    "- Trigrams\n",
    "- n-grams\n",
    "- TF-IDF Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "190d7c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = [\"this is a good movie\"]\n",
    "sent_2 = [\"this is not a good movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "f719ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "10b0e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [sent_1[0],sent_2[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9d4bdfce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "159a86ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 11,\n",
       " 'is': 2,\n",
       " 'good': 0,\n",
       " 'movie': 7,\n",
       " 'this is': 12,\n",
       " 'is good': 3,\n",
       " 'good movie': 1,\n",
       " 'this is good': 13,\n",
       " 'is good movie': 4,\n",
       " 'not': 8,\n",
       " 'is not': 5,\n",
       " 'not good': 9,\n",
       " 'this is not': 14,\n",
       " 'is not good': 6,\n",
       " 'not good movie': 10}"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a787bf3a",
   "metadata": {},
   "source": [
    "## TF-IDF Normalisation\n",
    "- avoids features that occur very often,because they contains less information\n",
    "- information decreases as the number of occurances accross diffrent type os documents\n",
    "- so we define another term - term-document-frequency which associated a weight with every term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "3433154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = \"this is good movie\"\n",
    "sent_2 = \"this was good movie\"\n",
    "sent_3 = \"this is not a good movie\"\n",
    "docs = [sent_1,sent_2,sent_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1e35333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "24dbaedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "168ac01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = tfidf.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "00e5b591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6990303272568005"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(vc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "cbf2b4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46333427, 0.59662724, 0.46333427, 0.        , 0.46333427,\n",
       "        0.        ],\n",
       "       [0.41285857, 0.        , 0.41285857, 0.        , 0.41285857,\n",
       "        0.69903033],\n",
       "       [0.3645444 , 0.46941728, 0.3645444 , 0.61722732, 0.3645444 ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "57dbe168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'was': 5, 'not': 3}"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb860b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
